{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aacaa130",
   "metadata": {},
   "source": [
    "# 1. SETUP AND IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5491683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import getpass\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# LangChain/LangGraph Components\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import MessagesState, StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# Utility\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"Please set your GOOGLE_API_KEY variable.\")\n",
    "\n",
    "# if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "#     raise ValueError(\"Please set your GOOGLE_API_KEY variable.\")\n",
    "\n",
    "# --- RAG Setup (Placeholder) ---\n",
    "PDF_PATH = \"data.pdf\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='app.log',\n",
    "    encoding=\"utf-8\",\n",
    "    filemode=\"a\",\n",
    "    format=\"%(levelname)s:%(name)s:%(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6dc1c",
   "metadata": {},
   "source": [
    "# 2 Compulsory tool: get_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_weather(date: str, location: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the weather forecast for a specific date and city.\n",
    "    \"\"\"\n",
    "    # Basic Error Management: Check for missing location\n",
    "    if not location:\n",
    "        error_msg = \"Error: Location is not provided\"\n",
    "        logging.error(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "    # Parse the date, raise error if not in the right format\n",
    "    try:\n",
    "        forecast_date = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        today = datetime.now().date()\n",
    "    except:\n",
    "        error_msg = \"Error: Invalid date format. The date MUST be in 'YYYY-MM-DD' format.\"\n",
    "        logging.error(error_msg)\n",
    "        return error_msg\n",
    "    \n",
    "    # Basic Error Management: Check if the forecast is too far in the future\n",
    "    if forecast_date > today + timedelta(days=7):\n",
    "        error_msg = f\"Error: Forecast for {date} is too far in the future. We can only predict the weather for the next 7 days.\"\n",
    "        logging.error(error_msg)\n",
    "        return error_msg\n",
    "    \n",
    "    # Mocked API Logic\n",
    "    if location.lower() in [\"vilafranca\", \"sitges\"] and forecast_date >= today:\n",
    "        day_of_week = forecast_date.weekday()\n",
    "        \n",
    "        if day_of_week in [5, 6]: # Weekend\n",
    "            weather = \"sunny\"\n",
    "            temp = \"28°C\"\n",
    "        elif day_of_week in [0, 1, 2]: # Start of week\n",
    "            weather = \"partly cloudy\"\n",
    "            temp = \"25°C\"\n",
    "        else: # Mid-week\n",
    "            weather = \"light rain\"\n",
    "            temp = \"22°C\"\n",
    "        logging.info(\"Weather data retrieved\")\n",
    "        return f\"The weather in {location} on {date} will be **{weather}** with a high of **{temp}**.\"\n",
    "    \n",
    "    elif forecast_date < today:\n",
    "        logging.info(\"Weather data retrieved\")\n",
    "        return f\"Historical weather in {location} on {date}: It was a warm, sunny day.\"\n",
    "        \n",
    "    else:\n",
    "        logging.info(\"Weather data retrieved\")\n",
    "        return f\"Weather forecast for {location} on {date}: Data unavailable, but expect typical Spanish weather.\"\n",
    "\n",
    "# List of all available tools\n",
    "TOOLS = [get_weather]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb400c",
   "metadata": {},
   "source": [
    "# 3. RAG implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0bb80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rag(pdf_path: str):\n",
    "    \"\"\"Loads, chunks, and indexes the PDF guide into a FAISS VectorStore.\"\"\"\n",
    "   \n",
    "    # 1. Load Document\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}. Using a placeholder document.\")\n",
    "        docs = [{\"page_content\": \"Vilafranca is considered the city of wine. Sitges is a beautiful coastal town known for its film festival, beaches, and historical sites like the Maricel Museum.\"}]\n",
    "        \n",
    "    # 2. Chunking Strategy (RecursiveCharacterTextSplitter is robust)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # 3. Embeddings (using paraphrase-multilingual-MiniLM-L12-v2 for the multilingual approach)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    \n",
    "    # 4. Vector Store (FAISS)\n",
    "    vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "    \n",
    "    # 5. Retriever\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    return retriever\n",
    "\n",
    "# Initialize the RAG Retriever\n",
    "RAG_RETRIEVER = setup_rag(PDF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c942f",
   "metadata": {},
   "source": [
    "# 4. Model and State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "llm = init_chat_model(\n",
    "    model=MODEL_NAME,\n",
    "    model_provider=\"google_genai\", \n",
    "    temperature=0.3,\n",
    "    max_tokens=2048\n",
    ").bind_tools(TOOLS) # Bind the get_weather tool\n",
    "\n",
    "# System Prompt to define the Agent's persona and instructions\n",
    "SYSTEM_PROMPT = SystemMessage(\n",
    "    content=(\n",
    "        \"You are a helpful and friendly Tourist Assistant for the Vilafranca and surrounding areas. \"\n",
    "        \"Your primary source of information is the provided RAG context about the local guide. \"\n",
    "        \"Always maintain context from the previous turns of the conversation. \"\n",
    "        f\"If the user asks about the weather, you MUST use the 'get_weather' tool, regardless of the date used. Use it even when the user has not specified a specific date to find the most appropiate date for a certain plan, considering that today is {datetime.now().date()}.\"\n",
    "        \"You MUST answer using the language the user used in their original prompt, not the language in the query passed to the RAG system (e.g., if the user asks in Spanish and the RAG system answers in Catalan, answer in Spanish),\"\n",
    "        \"even if the retrieved RAG context is in a different language (e.g., Catalan).\"\n",
    "        \"If the information is not in your RAG context or is not related with turism say that you do not know.\"\n",
    "        \"You MUST ALWAYS return the page numbers corresponding to each chunck of information used to deliver your answer to the user. Do so at the end of the message, like this: [Source: Page X, Page Y]\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109587c6",
   "metadata": {},
   "source": [
    "Since we want to be able to retrieve the context from the RAG, we add an extra field inside the State of our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f51938",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TouristAgentState(MessagesState):\n",
    "    retrieved_documents: List[Document]\n",
    "    transformed_query: str\n",
    "    \n",
    "AgentState = TouristAgentState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04de394",
   "metadata": {},
   "source": [
    "# 5. Nodes and edges: Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da5213",
   "metadata": {},
   "source": [
    "To allow a better performance, we will use an auxiliar llm to transform the user's query to substitue any confusing term or structure considering the chat's history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ba7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_PROMPT_TEMPLATE = \\\n",
    "\"\"\"You are an agent which sole goal is to transform a given query using the chat history between a user and an AI. \n",
    "The transformation has to make the query not depend on context and substitue any pronoun or ambiguity for their corresponding keyword words that appeared before in the chat.\n",
    "\n",
    "Query: \\n{query}\n",
    "Chat history: \\n{chat_history}\"\"\"\n",
    "\n",
    "rewrite_prompt = ChatPromptTemplate.from_template(REWRITE_PROMPT_TEMPLATE)\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "aux_llm = init_chat_model(\n",
    "    model=MODEL_NAME,\n",
    "    model_provider=\"google_genai\", \n",
    "    temperature=0.3,\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "rewrite_chain = rewrite_prompt | aux_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7702784",
   "metadata": {},
   "source": [
    "Once our rewriting feature has been defined, we have to create the main functions which will consitute the nodes of our graph: One function to rewrite the user's query, one to retrieve the information from the RAG and one to use both the query and the context to give a proper answer. (The weather node will be defined using the previously defined function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a1b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_history(messages: list) -> str:\n",
    "    \"\"\"Receives a list of messages corresponding to a conversation between a user (human) and an agent (ai) and returns a formatted string respresenting the last 5 interactions\"\"\"\n",
    "    history_str = \"\"\n",
    "    for msg in messages[-5:]:\n",
    "        if msg.role == \"user\":\n",
    "            history_str += f\"Human:\\n{msg.content}\"\n",
    "        if msg.role == \"assistant\":\n",
    "            history_str += f\"AI:\\n{msg.content}\"\n",
    "    return history_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e81e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query_node(state: AgentState) -> dict:\n",
    "    \"\"\"Catches last message and all previous messages in chat history and rewrites the last message into a more convenient format\"\"\"\n",
    "    current_query = state['messages'][-1]\n",
    "    chat_history = format_chat_history(state['messages'][:-1])\n",
    "\n",
    "    response = rewrite_chain.invoke({\n",
    "        'query': current_query,\n",
    "        'chat_history': chat_history\n",
    "    })\n",
    "\n",
    "    return {'transformed_query': response}\n",
    "\n",
    "def retrieve_context_node(state: AgentState) -> dict:\n",
    "    \"\"\"Returns the most important chunkcs of information from the RAG system considering the transformed query\"\"\"\n",
    "    docs = RAG_RETRIEVER.invoke(state['transformed_query'])\n",
    "    return {\"retrieved_documents\": docs}\n",
    "\n",
    "def agent_node(state: AgentState) -> dict:\n",
    "    \"\"\"Invokes the LLM considering its role, the context from the RAG system and previous messages\"\"\"\n",
    "    context_parts = [f\"--Source [Page {doc.metadata.page}] -- {doc.page_content} -- End Source --\" for doc in state[\"retrieved_documents\"]]\n",
    "    context = \"\\n\\n--- RAG Context ---\\n\" + \"\\n\".join(context_parts)\n",
    "    messages_with_context = [SYSTEM_PROMPT] + [SystemMessage(content=context)] + state[\"messages\"]\n",
    "    response = llm.invoke(messages_with_context)\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d51378f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent graph compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "# B. Node 2: Tool Node (weather tool)\n",
    "tool_node = ToolNode(TOOLS)\n",
    "\n",
    "# C. Define the Graph\n",
    "graph_builder = StateGraph(AgentState)\n",
    "\n",
    "# Add Nodes\n",
    "graph_builder.add_node(\"rewriter\", rewrite_query_node)\n",
    "graph_builder.add_node(\"rag\", retrieve_context_node)\n",
    "graph_builder.add_node(\"agent\", agent_node)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set Entry Point\n",
    "graph_builder.set_entry_point(\"rewriter\")\n",
    "\n",
    "# Define Edges/Transitions\n",
    "graph_builder.add_edge(\"rewriter\", \"rag\")\n",
    "graph_builder.add_edge(\"rag\", \"agent\")\n",
    "# LangGraph's prebuilt tools_condition checks if the LLM requested a function call.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"tools\",  # If tool call, go to 'tools' node\n",
    "        END: END,          # If no tool call, finish the conversation\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. From 'tools' back to 'agent'\n",
    "# After the tool runs, its output is returned to the 'agent' node for synthesis into a final answer.\n",
    "graph_builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver() # To maintain state/context across turns\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "print(\"Agent graph compiled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e8814",
   "metadata": {},
   "source": [
    "# 6. EXECUTION AND DEMONSTRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43aaa53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the Agent (using a unique thread ID for multi-turn dialogue)\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "def run_agent(prompt: str, return_context=False, return_time=False, has_used_tool=False):\n",
    "    time.sleep(5)\n",
    "    t0 = datetime.now()\n",
    "    \"\"\"Helper function to run the agent and print the final output.\"\"\"\n",
    "    print(f\"\\n==================================\\n User: {prompt}\\n==================================\")\n",
    "    \n",
    "    # Prepare the input message\n",
    "    input_message = {\"messages\": [{\"role\": \"user\", \"content\": prompt}]}\n",
    "    \n",
    "    # Run the graph\n",
    "    final_state = graph.invoke(input_message, config=config)\n",
    "\n",
    "    # Print answer from assistant\n",
    "    final_answer = final_state[\"messages\"][-1].content\n",
    "    print(f\"\\n==================================\\n Assistant: {final_answer}\\n==================================\")\n",
    "    \n",
    "    duration = datetime.now() -t0\n",
    "    results = {\n",
    "        \"answer\": final_answer\n",
    "    }\n",
    "    if return_context:\n",
    "        if final_state.get(\"retrieved_documents\"):\n",
    "            results['context'] = [doc.page_content for doc in final_state[\"retrieved_documents\"]]\n",
    "        else:\n",
    "            results['context'] = []\n",
    "    if return_time:\n",
    "        results[\"duration\"] = duration\n",
    "\n",
    "    if has_used_tool:\n",
    "        tool_was_called = False\n",
    "        for msg in final_state[\"messages\"]:\n",
    "            if isinstance(msg, ToolMessage):\n",
    "                tool_was_called = True\n",
    "                break\n",
    "        results['has_used_tool'] = tool_was_called\n",
    "\n",
    "    return results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bbff305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      " User: Quiero ir a Vilafranca. ¿Qué museos hay allí?\n",
      "==================================\n",
      "\n",
      "==================================\n",
      " Assistant: En Vilafranca del Penedès, puedes visitar el Museu del Vi.\n",
      "==================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'En Vilafranca del Penedès, puedes visitar el Museu del Vi.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"Quiero ir a Vilafranca. ¿Qué museos hay allí?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec97663d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      " User: ¿Qué me puedes decir sobre ese museo?\n",
      "==================================\n",
      "\n",
      "==================================\n",
      " Assistant: El Museu del Vi de Vilafranca, conocido como Vinseum, tiene sus orígenes hace casi 100 años y se creó oficialmente en el año 2000. Actualmente, se encuentra en fase de ampliación y es una de las áreas con mayor potencial de crecimiento en la región.\n",
      "==================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'El Museu del Vi de Vilafranca, conocido como Vinseum, tiene sus orígenes hace casi 100 años y se creó oficialmente en el año 2000. Actualmente, se encuentra en fase de ampliación y es una de las áreas con mayor potencial de crecimiento en la región.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"¿Qué me puedes decir sobre ese museo?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497227ed",
   "metadata": {},
   "source": [
    "Invocación correcta (Éxito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d33d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      " User: Quiero ir al museo cuando llueva. ¿Cuándo me recomeindas que vaya?\n",
      "==================================\n",
      "\n",
      "==================================\n",
      " Assistant: No dispongo de información sobre cuándo lloverá en Vilafranca del Penedès.\n",
      "==================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'No dispongo de información sobre cuándo lloverá en Vilafranca del Penedès.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"Quiero ir al museo cuando llueva. ¿Cuándo me recomeindas que vaya?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb76d70",
   "metadata": {},
   "source": [
    "Invocación con fallo de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efb1d2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      " User: ¿Y la predicción para 11-2025-11?\n",
      "==================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HumanMessage' object has no attribute 'role'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m¿Y la predicción para 11-2025-11?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mrun_agent\u001b[39m\u001b[34m(prompt, return_context, return_time, has_used_tool)\u001b[39m\n\u001b[32m     12\u001b[39m input_message = {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}]}\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run the graph\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m final_state = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Print answer from assistant\u001b[39;00m\n\u001b[32m     18\u001b[39m final_answer = final_state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Master IA, DevOps and Cloud\\Module 06 LLM\\Entrega Final\\hands_on_final_exercise\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3094\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3091\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3092\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3094\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3100\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3102\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3103\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3104\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Master IA, DevOps and Cloud\\Module 06 LLM\\Entrega Final\\hands_on_final_exercise\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2679\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2677\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2678\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2684\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2688\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2689\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Master IA, DevOps and Cloud\\Module 06 LLM\\Entrega Final\\hands_on_final_exercise\\venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Master IA, DevOps and Cloud\\Module 06 LLM\\Entrega Final\\hands_on_final_exercise\\venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Master IA, DevOps and Cloud\\Module 06 LLM\\Entrega Final\\hands_on_final_exercise\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Master IA, DevOps and Cloud\\Module 06 LLM\\Entrega Final\\hands_on_final_exercise\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mrewrite_query_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Catches last message and all previous messages in chat history and rewrites the last message into a more convenient format\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m current_query = state[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m chat_history = \u001b[43mformat_chat_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m response = rewrite_chain.invoke({\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m: current_query,\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mchat_history\u001b[39m\u001b[33m'\u001b[39m: chat_history\n\u001b[32m      9\u001b[39m })\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33mtransformed_query\u001b[39m\u001b[33m'\u001b[39m: response}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mformat_chat_history\u001b[39m\u001b[34m(messages)\u001b[39m\n\u001b[32m      3\u001b[39m history_str = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages[:-\u001b[32m5\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmsg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrole\u001b[49m == \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      6\u001b[39m         history_str += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHuman:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m msg.role == \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Master IA, DevOps and Cloud\\Module 06 LLM\\Entrega Final\\hands_on_final_exercise\\venv\\Lib\\site-packages\\pydantic\\main.py:1026\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1025\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'HumanMessage' object has no attribute 'role'",
      "During task with name 'rewriter' and id 'eb14735c-a813-2fa6-8152-58a71a507aa5'"
     ]
    }
   ],
   "source": [
    "\n",
    "run_agent(\"¿Y la predicción para 11-2025-11?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f8057",
   "metadata": {},
   "source": [
    "Invocación con fallo lógico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91239b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(\"¿Y la predicción para 2030-01-01?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2821ed0",
   "metadata": {},
   "source": [
    "# 7. Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e807997a",
   "metadata": {},
   "source": [
    "We prepare a series of questions for the rag and will evaluate different metrics considering the answers and the information retrieved from the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99955750",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompts = [\n",
    "    \"¿Que castillos hay por el Penedés?\",\n",
    "    \"¿Que bodegas mas famosas hay por el Penedés?\",\n",
    "    \"En que región del Penedés/Garraf hay mas hoteles?\",\n",
    "    \"¿Donde hay más segundas residencias en el area Penedés/Anoia/Garraf?\",\n",
    "    \"¿Donde hay hacimientos arqueológicos en la zona Penedés/Anoia/Garraf?\",\n",
    "    \"¿En que consiste la ruta del vino?\",\n",
    "    \"¿Hay muchos festivales relacionados con el vino en la zona Penedés/Anoia/Garraf?\",\n",
    "    \"¿Donde puedo acampar en Penedés/Anoia/Garraf?\",\n",
    "    \"¿Cual es la mejor ruta caminando cerca (A 20 km máximo) de Vilafranca del Penedés?\",\n",
    "    \"¿Que tipo de turismo es el más famoso en la zona de Penedés/Anoia/Garraf?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6936df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_prompts = [\n",
    "    \"Que tiempo hace mañana en Sitges?\",\n",
    "    \"Que tiempo hace mañana en Vilafranca del Penedés?\",\n",
    "    \"Llueve dentro de tres días en Vilafranca del Penedés?\",\n",
    "    \"Cual es el mejor día para pasear por Vilafranca del Penedés?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8e1124",
   "metadata": {},
   "source": [
    "## Mesure 1: Context precision:\n",
    "\n",
    "context_precision = number of relevant chuncks (useful chuncks) / number of retrieved chunks\n",
    "\n",
    "This metric will be evaluated manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b7cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = []\n",
    "answer_list = []\n",
    "duration_list = []\n",
    "for num_prompt, prompt in enumerate(rag_prompts):\n",
    "    result = run_agent(prompt , return_context=True, return_time=True)\n",
    "    answer_list.append(result['answer'])\n",
    "    context_list.append(result['context'])\n",
    "    duration_list.append(result['duration'])\n",
    "    print(f\"For prompt #{num_prompt} {prompt} context retrieved is:\\n\" + \"\\n\\n\".join([\"Context #\" + str(num_context + 1) + \": \" + single_context for num_context, single_context in enumerate(context)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f15b3",
   "metadata": {},
   "source": [
    "Summary of relevant chuncks retrieved for the answer for each prompt:\n",
    "\n",
    "Prompt 1: 2/3\n",
    "\n",
    "Prompt 2: 1/3\n",
    "\n",
    "Prompt 3: 1/3\n",
    "\n",
    "Prompt 4: 2/3\n",
    "\n",
    "Prompt 5: 1/3\n",
    "\n",
    "Prompt 6: 1/3\n",
    "\n",
    "Prompt 7: 1/3\n",
    "\n",
    "Prompt 8: 1/3\n",
    "\n",
    "Prompt 9: 2/3\n",
    "\n",
    "Prompt 10: 1/3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1348f",
   "metadata": {},
   "source": [
    "context_precision = 13/30; aprox 43%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a3593",
   "metadata": {},
   "source": [
    "Change chunk_size to a lower value and rerun everything and reevaluate the result manually again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e099c1",
   "metadata": {},
   "source": [
    "## Mesure 2: Faithfuness: \n",
    "\n",
    "Faithfuness = number of sentences based on the retrieved information / number of sentences in the answer of the llm\n",
    "\n",
    "We allow a llm to decide whether each sentence in the generated answer is based in any retrieved chunck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "assist_llm = init_chat_model(\n",
    "    model=MODEL_NAME,\n",
    "    model_provider=\"google_genai\", \n",
    "    temperature=0.3,\n",
    "    max_tokens=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bbc15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = SystemMessage(content=\"You are an AI whose sole purpose is to decide if a sentence's affirmation is based on a series of chuncks of information. Give only the answers as 1 if True or or 0 if False.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfuness_list = []\n",
    "for idx, answer in enumerate(answer_list):\n",
    "    results = []\n",
    "    for sentence in answer.split(\".\"):\n",
    "        time.sleep(5)\n",
    "        human_msg = HumanMessage(f\"Sentence: {sentence}\\nChunks of information: \" + \"\\n\".join([\"Chunk #\" + str(num_context + 1) + \": \" + single_context for num_context, single_context in enumerate(context_list[idx])]))\n",
    "        messages = [system_msg, human_msg]\n",
    "        result = assist_llm.invoke(messages)\n",
    "        try:\n",
    "            numerical_answer = int(result.content)\n",
    "            results.append(numerical_answer)\n",
    "        except ValueError:\n",
    "            pass \n",
    "    faithfuness = sum(results)/len(results)\n",
    "    faithfuness_list.append(faithfuness)\n",
    "    print(f\"For prompt #{idx+1} faitfhfulness score is {faithfuness}\")\n",
    "print(f\"Total faithfulness is {sum(faithfuness_list)/len(faithfuness_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546959e",
   "metadata": {},
   "source": [
    "## Mesure 3: RAG delay\n",
    "\n",
    "We will visualize the ammount of time needed for the agent to deal with each prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7bb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = []\n",
    "for prompt in rag_prompts:\n",
    "    tokens_list.append(llm.get_num_tokens(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39d515a",
   "metadata": {},
   "source": [
    "Visualization of number of tokens in prompt vs ammount of time taken for agent to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299edb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tokens_list, duration_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa021a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for tool_prompt in tool_prompts:\n",
    "    result = run_agent(tool_prompt, has_used_tool=True)\n",
    "    result_list.append(result['has_used_tool'])\n",
    "print(f\"Precision of correct tool calls is: {sum(result_list)/len(result_list):.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782cba87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
